# ğŸ§  Mindoc â€” Private Offline RAG Assistant

Mindoc is a **fully offline, privacy-first AI assistant** that lets you **chat with your PDFs and PPTs** â€” without internet, API keys, or cloud services.  
Everything runs **entirely on your local CPU** using Small Language Models (SLMs).

---

## ğŸš€ Key Features

### ğŸ”’ 100% Offline
- No OpenAI  
- No cloud dependencies  
- No data leaves your device  
- All models stored locally  

### ğŸ“„ Multi-Document Ingestion
- Upload **multiple PDFs and PPTs**  
- Fully private, local processing  
- Fast and accurate extraction  

### ğŸ§  Local LLM
Powered by **LaMini-Flan-T5 (248M)** optimized for CPU inference.

### ğŸ” Hybrid Search Engine
**Semantic Vector Search + Cross-Encoder Reranking**

- Vector Model: `all-MiniLM-L6-v2`  
- Reranker: `ms-marco-MiniLM-L12-v2`  

### âš¡ Dual Search Modes
- **Quick Mode:** Fast, short answers (Top-2 docs)  
- **Deep Research:** Multi-doc reasoning using Map-Reduce  

### ğŸ”— Smart Citations
- Evidence-based answers  
- Click on a citation â†’ open PDF â†’ auto-scroll to exact page  

---

## ğŸ› ï¸ Technical Architecture

### 1. Ingestion Pipeline
- **Loader:** PyMuPDFLoader  
- **Chunking:** RecursiveCharacterTextSplitter  
  - Chunk size: 1000 chars  
  - Overlap: 200 chars  
- **Embeddings:** SentenceTransformer (384-dim)  
- **Storage:** ChromaDB (Local persistence)

### 2. Retrieval & Generation
1. Retrieve top-10 chunks with vector search  
2. Re-rank with cross-encoder, keep best 3  
3. Feed context â†’ LaMini LLM â†’ generate answer  

---
## ğŸ—ï¸ System Architecture
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Files     â”‚
           â”‚ PDF / PPTX  â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Document Loader â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚Chunks
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Embeddings (Local Model)â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚Vectors
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Vector Store      â”‚
        â”‚ FAISS / Chroma    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ RAG Pipeline       â”‚
        â”‚ (Retrieve + LLM)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  FastAPI    â”‚
          â”‚  /query     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
---

## ğŸ“¦ Installation Guide

### Prerequisites
- Python **3.10+** (3.12 recommended)  
- Node.js & npm  

---

## ğŸ”§ Backend Setup (FastAPI)

```bash
cd backend

# Virtual environment
python -m venv venv
source venv/bin/activate    # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download models (run once)
python download_model.py
python download_reranker.py

# Start backend server
uvicorn app.main:app --reload
```
## ğŸ¨ Frontend Setup (React + Vite)

```bash
cd frontend

npm install
npm run dev
```

## ğŸ–¥ï¸ Usage

### **Upload Documents**
- Drag & drop PDFs  
- Supports batch uploads  
- Wait for **â€œâœ… Indexedâ€** confirmation  

### **Chat with Your Documents**

#### âš¡ **Quick Mode**
- Fast  
- Lightweight  
- Best for direct questions  

#### ğŸ§  **Deep Research**
- Reads many chunks  
- Map-Reduce summarization  
- Great for reports & summaries  

### **Verify Sources**
- Each answer includes clickable citations  
- Opens full PDF and auto-scrolls to correct page  

---

## ğŸ“‚ Project Structure

```
mindoc/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”œâ”€â”€ rag/
â”‚ â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â””â”€â”€ main.py
â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”œâ”€â”€ chroma/
â”‚ â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â””â”€â”€ uploads/
â”‚ â”œâ”€â”€ download_model.py
â”‚ â”œâ”€â”€ download_reranker.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ frontend/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ App.jsx
â”‚ â”œâ”€â”€ App.css
â”‚ â””â”€â”€ main.jsx
â””â”€â”€ package.json

```

## â— Troubleshooting

### **sqlite3 errors (Python 3.12)**
Install SQLite shim:

```bash
pip install pysqlite3-binary
```
### **Context Window Errors**
Long chunks â†’ crash.
Fixed by enabling:
```bash
truncation=True
```
### **500 Search Errors**
Usually caused by a missing reranker model.
Run again:
```bash
python download_reranker.py
```
---
## ğŸ”® Future Roadmap

- [ ] OCR for scanned documents  
- [ ] Model switching (LaMini â†” Phi-2 â†” Qwen 0.5B)  
- [ ] Persistent conversation history  
- [ ] Voice mode (offline ASR)
---
